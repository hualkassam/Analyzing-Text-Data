{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data Using Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple \\\n",
    "of sentences with punctuations to see it in action.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenizer:\n",
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(\"Sentence tokenizer:\")\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"Word tokenizer:\")\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word punct tokenizer:\n",
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new WordPunct tokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(\"Word punct tokenizer:\")\n",
    "print(word_punct_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table', 'probably', 'wolves', 'playing', 'is', \n",
    "        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different stemmers\n",
    "stemmers = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
    "stemmer_porter = PorterStemmer()\n",
    "stemmer_lancaster = LancasterStemmer()\n",
    "stemmer_snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             WORD          PORTER       LANCASTER        SNOWBALL \n",
      "\n",
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "formatted_row = '{:>16}' * (len(stemmers) + 1)\n",
    "print('\\n', formatted_row.format('WORD', *stemmers), '\\n')\n",
    "for word in words:\n",
    "    stemmed_words = [stemmer_porter.stem(word), \n",
    "            stemmer_lancaster.stem(word), stemmer_snowball.stem(word)]\n",
    "    print(formatted_row.format(word, *stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Text to Its Base Form Using Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['table', 'probably', 'wolves', 'playing', 'is', \n",
    "        'dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different lemmatizers\n",
    "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "lemmatizer_wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      "\n",
      "                   table                   table                   table\n",
      "                probably                probably                probably\n",
      "                  wolves                    wolf                  wolves\n",
      "                 playing                 playing                    play\n",
      "                      is                      is                      be\n",
      "                     dog                     dog                     dog\n",
      "                     the                     the                     the\n",
      "                 beaches                   beach                   beach\n",
      "                grounded                grounded                  ground\n",
      "                  dreamt                  dreamt                   dream\n",
      "                envision                envision                envision\n"
     ]
    }
   ],
   "source": [
    "formatted_row = '{:>24}' * (len(lemmatizers) + 1)\n",
    "print('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
    "for word in words:\n",
    "    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'),\n",
    "           lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
    "    print(formatted_row.format(word, *lemmatized_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing Text Using Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a text into chunks \n",
    "def splitter(data, num_words):\n",
    "    words = data.split(' ')\n",
    "    output = []\n",
    "\n",
    "    cur_count = 0\n",
    "    cur_words = []\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "\n",
    "    output.append(' '.join(cur_words) )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # Read the data from the Brown corpus\n",
    "    data = ' '.join(brown.words()[:10000])\n",
    "\n",
    "    # Number of words in each chunk \n",
    "    num_words = 1700\n",
    "\n",
    "    chunks = []\n",
    "    counter = 0\n",
    "\n",
    "    text_chunks = splitter(data, num_words)\n",
    "\n",
    "    print(\"Number of text chunks =\", len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from chunking import splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
      " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
      " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
      " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
      " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
      " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
      " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
      " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n",
      "Document term matrix:\n",
      "\n",
      "         Word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4 \n",
      "\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # Read the data from the Brown corpus\n",
    "    data = ' '.join(brown.words()[:10000])\n",
    "\n",
    "    # Number of words in each chunk \n",
    "    num_words = 2000\n",
    "\n",
    "    chunks = []\n",
    "    counter = 0\n",
    "\n",
    "    text_chunks = splitter(data, num_words)\n",
    "\n",
    "    for text in text_chunks:\n",
    "        chunk = {'index': counter, 'text': text}\n",
    "        chunks.append(chunk)\n",
    "        counter += 1\n",
    "\n",
    "    # Extract document term matrix\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=5, max_df=.95)\n",
    "    doc_term_matrix = vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "    print(\"Vocabulary:\")\n",
    "    print(vocab)\n",
    "\n",
    "    print(\"Document term matrix:\")\n",
    "    chunk_names = ['Chunk-0', 'Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4']\n",
    "    formatted_row = '{:>12}' * (len(chunk_names) + 1)\n",
    "    print('\\n', formatted_row.format('Word', *chunk_names), '\\n')\n",
    "    for word, item in zip(vocab, doc_term_matrix.T):\n",
    "        # 'item' is a 'csr_matrix' data structure\n",
    "        output = [str(x) for x in item.data]\n",
    "        print(formatted_row.format(word, *output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', \n",
    "        'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', \n",
    "        'sci.space': 'Space'}\n",
    "training_data = fetch_20newsgroups(subset='train', \n",
    "        categories=category_map.keys(), shuffle=True, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training data: (2968, 40605)\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_termcounts = vectorizer.fit_transform(training_data.data)\n",
    "print(\"Dimensions of training data:\", X_train_termcounts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "input_data = [\n",
    "    \"The curveballs of right handed pitchers tend to curve to the left\", \n",
    "    \"Caesar cipher is an ancient form of encryption\",\n",
    "    \"This two-wheeler is really good on slippery roads\"\n",
    "]\n",
    "\n",
    "# tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training_data.target)\n",
    "X_input_termcounts = vectorizer.transform(input_data)\n",
    "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: The curveballs of right handed pitchers tend to curve to the left \n",
      "Predicted category: Baseball\n",
      "\n",
      "Input: Caesar cipher is an ancient form of encryption \n",
      "Predicted category: Cryptography\n",
      "\n",
      "Input: This two-wheeler is really good on slippery roads \n",
      "Predicted category: Motorcycles\n"
     ]
    }
   ],
   "source": [
    "# Predict the output categories\n",
    "predicted_categories = classifier.predict(X_input_tfidf)\n",
    "\n",
    "# Print the outputs\n",
    "for sentence, category in zip(input_data, predicted_categories):\n",
    "    print('\\nInput:', sentence, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Gender of a Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('names')\n",
    "\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the input word\n",
    "def gender_features(word, num_letters=2):\n",
    "    return {'feature': word[-num_letters:].lower()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of letters: 1\n",
      "Accuracy ==> 76.2%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "\n",
      "Number of letters: 2\n",
      "Accuracy ==> 78.60000000000001%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "\n",
      "Number of letters: 3\n",
      "Accuracy ==> 76.6%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n",
      "\n",
      "Number of letters: 4\n",
      "Accuracy ==> 70.8%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # Extract labeled names\n",
    "    labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "            [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "    random.seed(7)\n",
    "    random.shuffle(labeled_names)\n",
    "    input_names = ['Leonardo', 'Amy', 'Sam']\n",
    "\n",
    "    # Sweeping the parameter space\n",
    "    for i in range(1, 5):\n",
    "        print('\\nNumber of letters:', i)\n",
    "        featuresets = [(gender_features(n, i), gender) for (n, gender) in labeled_names]\n",
    "        train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "        classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "        # Print classifier accuracy\n",
    "        print('Accuracy ==>', str(100 * nltk_accuracy(classifier, test_set)) + str('%'))\n",
    "\n",
    "        # Predict outputs for new inputs\n",
    "        for name in input_names:\n",
    "            print(name, '==>', classifier.classify(gender_features(name, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Sentiment of a Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/hussam/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(word_list):\n",
    "    return dict([(word, True) for word in word_list])\n",
    "\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training datapoints: 1600\n",
      "Number of test datapoints: 400\n",
      "Accuracy of the classifier: 0.735\n",
      "Top 10 most informative words:\n",
      "outstanding\n",
      "insulting\n",
      "vulnerable\n",
      "ludicrous\n",
      "uninvolving\n",
      "astounding\n",
      "avoids\n",
      "fascination\n",
      "affecting\n",
      "darker\n",
      "Predictions:\n",
      "Review: It is an amazing movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.61\n",
      "Review: This is a dull movie. I would never recommend it to anyone.\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.77\n",
      "Review: The cinematography is pretty great in this movie\n",
      "Predicted sentiment: Positive\n",
      "Probability: 0.67\n",
      "Review: The direction was terrible and the story was all over the place\n",
      "Predicted sentiment: Negative\n",
      "Probability: 0.63\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # Load positive and negative reviews  \n",
    "    positive_fileids = movie_reviews.fileids('pos')\n",
    "    negative_fileids = movie_reviews.fileids('neg')\n",
    "     \n",
    "    features_positive = [(extract_features(movie_reviews.words(fileids=[f])), \n",
    "            'Positive') for f in positive_fileids]\n",
    "    features_negative = [(extract_features(movie_reviews.words(fileids=[f])), \n",
    "            'Negative') for f in negative_fileids]\n",
    "     \n",
    "    # Split the data into train and test (80/20)\n",
    "    threshold_factor = 0.8\n",
    "    threshold_positive = int(threshold_factor * len(features_positive))\n",
    "    threshold_negative = int(threshold_factor * len(features_negative))\n",
    "     \n",
    "    features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]\n",
    "    features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]  \n",
    "    print(\"Number of training datapoints:\", len(features_train))\n",
    "    print(\"Number of test datapoints:\", len(features_test))\n",
    "     \n",
    "    # Train a Naive Bayes classifier\n",
    "    classifier = NaiveBayesClassifier.train(features_train)\n",
    "    print(\"Accuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
    "\n",
    "    print(\"Top 10 most informative words:\")\n",
    "    for item in classifier.most_informative_features()[:10]:\n",
    "        print(item[0])\n",
    "\n",
    "    # Sample input reviews\n",
    "    input_reviews = [\n",
    "        \"It is an amazing movie\", \n",
    "        \"This is a dull movie. I would never recommend it to anyone.\",\n",
    "        \"The cinematography is pretty great in this movie\", \n",
    "        \"The direction was terrible and the story was all over the place\" \n",
    "    ]\n",
    "\n",
    "    print(\"Predictions:\")\n",
    "    for review in input_reviews:\n",
    "        print(\"Review:\", review)\n",
    "        probdist = classifier.prob_classify(extract_features(review.split()))\n",
    "        pred_sentiment = probdist.max()\n",
    "        print(\"Predicted sentiment:\", pred_sentiment) \n",
    "        print(\"Probability:\", round(probdist.prob(pred_sentiment), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Patterns in Text Using Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import models, corpora\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load input data\n",
    "def load_data(input_file):\n",
    "    data = []\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line[:-1])\n",
    "\n",
    "    return data\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to preprocess text\n",
    "class Preprocessor(object):\n",
    "    # Initialize various operators\n",
    "    def __init__(self):\n",
    "        # Create a regular expression tokenizer\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        # get the list of stop words \n",
    "        self.stop_words_english = stopwords.words('english')\n",
    "\n",
    "        # Create a Snowball stemmer \n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        \n",
    "    # Tokenizing, stop word removal, and stemming\n",
    "    def process(self, input_text):\n",
    "        # Tokenize the string\n",
    "        tokens = self.tokenizer.tokenize(input_text.lower())\n",
    "\n",
    "        # Remove the stop words \n",
    "        tokens_stopwords = [x for x in tokens if not x in self.stop_words_english]\n",
    "        \n",
    "        # Perform stemming on the tokens \n",
    "        tokens_stemmed = [self.stemmer.stem(x) for x in tokens_stopwords]\n",
    "\n",
    "        return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most contributing words to the topics:\n",
      "Topic 0 ==> 0.061*\"need\" + 0.037*\"order\" + 0.036*\"modern\" + 0.036*\"good\"\n",
      "Topic 1 ==> 0.053*\"need\" + 0.032*\"polici\" + 0.032*\"club\" + 0.032*\"develop\"\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    # File containing linewise input data \n",
    "    input_file = 'Python-Machine-Learning-Cookbook-Second-Edition-master/Chapter07/data_topic_modeling.txt'\n",
    "\n",
    "    # Load data\n",
    "    data = load_data(input_file)\n",
    "\n",
    "    # Create a preprocessor object\n",
    "    preprocessor = Preprocessor()\n",
    "\n",
    "    # Create a list for processed documents\n",
    "    processed_tokens = [preprocessor.process(x) for x in data]\n",
    "\n",
    "    # Create a dictionary based on the tokenized documents\n",
    "    dict_tokens = corpora.Dictionary(processed_tokens)\n",
    "        \n",
    "    # Create a document-term matrix\n",
    "    corpus = [dict_tokens.doc2bow(text) for text in processed_tokens]\n",
    "\n",
    "    # Generate the LDA model based on the corpus we just created\n",
    "    num_topics = 2\n",
    "    num_words = 4\n",
    "    ldamodel = models.ldamodel.LdaModel(corpus, \n",
    "            num_topics=num_topics, id2word=dict_tokens, passes=25)\n",
    "\n",
    "    print(\"Most contributing words to the topics:\")\n",
    "    for item in ldamodel.print_topics(num_topics=num_topics, num_words=num_words):\n",
    "        print (\"Topic\", item[0], \"==>\", item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "Text = nlp(u'We catched fish, and talked, and we took a swim now and then to keep off sleepiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We -PRON- PRON PRP nsubj Xx True True\n",
      "catched catch VERB VBD ROOT xxxx True False\n",
      "fish fish NOUN NN dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "talked talk VERB VBD conj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "we -PRON- PRON PRP nsubj xx True True\n",
      "took take VERB VBD conj xxxx True False\n",
      "a a DET DT det x True True\n",
      "swim swim NOUN NN dobj xxxx True False\n",
      "now now ADV RB advmod xxx True True\n",
      "and and CCONJ CC cc xxx True True\n",
      "then then ADV RB advmod xxxx True True\n",
      "to to PART TO aux xx True True\n",
      "keep keep VERB VB conj xxxx True True\n",
      "off off PART RP prt xxx True True\n",
      "sleepiness sleepiness NOUN NN dobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in Text:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package abc to /home/hussam/nltk_data...\n",
      "[nltk_data]   Package abc is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('abc')\n",
    "import gensim\n",
    "from nltk.corpus import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10363, size=100, alpha=0.025)\n",
      "[('law', 0.9405243396759033), ('general', 0.9295349717140198), ('policy', 0.9259950518608093), ('agriculture', 0.9259352684020996), ('media', 0.9199274182319641), ('discussion', 0.9188932776451111), ('practice', 0.9170894622802734), ('board', 0.910751223564148), ('reservoir', 0.9100625514984131), ('Crean', 0.9095194935798645)]\n"
     ]
    }
   ],
   "source": [
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "print(model)\n",
    "X= list(model.wv.vocab)\n",
    "data=model.wv.most_similar('science')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Learning for Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Python-Machine-Learning-Cookbook-Second-Edition-master/Chapter07/'\n",
    "\n",
    "df = pd.read_csv(path + 'spam.csv', sep=',',header=None, encoding='latin-1')\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(df.iloc[1:, 1],df.iloc[1:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "classifier = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam' 'ham']\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(['Customer Loyalty Offer:The NEW Nokia6650 Mobile from ONLY å£10 at TXTAUCTION!',\n",
    "                               'Hi Dear how long have we not heard.'] )\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3.6)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
